{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8fcd1a",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c6b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow text classifier — copy into a notebook cell\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b07cc0",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09016a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "890fa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "DATASET_PATH = \"./dataset/filtered_data.csv\"\n",
    "TEXT_COL = \"ingredients_text\"\n",
    "NUM_COLS = [\n",
    "    \"energy-kcal_100g\",\n",
    "    \"fat_100g\",\n",
    "    \"saturated-fat_100g\",\n",
    "    \"carbohydrates_100g\",\n",
    "    \"sugars_100g\",\n",
    "    \"proteins_100g\",\n",
    "    \"fiber_100g\",\n",
    "    \"salt_100g\",\n",
    "    \"sodium_100g\"\n",
    "]\n",
    "LABEL_COL = \"healthy_level\"\n",
    "MAX_TOKENS = 20000\n",
    "SEQ_LEN = 200\n",
    "EMBED_DIM = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d2d6f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45987c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (42790, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients_text</th>\n",
       "      <th>energy-kcal_100g</th>\n",
       "      <th>fat_100g</th>\n",
       "      <th>saturated-fat_100g</th>\n",
       "      <th>carbohydrates_100g</th>\n",
       "      <th>sugars_100g</th>\n",
       "      <th>proteins_100g</th>\n",
       "      <th>fiber_100g</th>\n",
       "      <th>salt_100g</th>\n",
       "      <th>sodium_100g</th>\n",
       "      <th>healthy_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HONIG stillende Frauen nicht geeignet. D bestr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sojaproteinisolat, Weizen - protein, Kaffee-Ex...</td>\n",
       "      <td>358.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.70</td>\n",
       "      <td>76.0</td>\n",
       "      <td>10.714286</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Water, Leptospermum Scoparium Mel (Manuka Hone...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.70</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.025</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Farine de blé 27%, chocolat au lait 18% (sucre...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>31.00</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.192</td>\n",
       "      <td>Not Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Madeleines ChocoNoir - Madeleines nappées de c...</td>\n",
       "      <td>389.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>6.48</td>\n",
       "      <td>35.2</td>\n",
       "      <td>1.85</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.352</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ingredients_text  energy-kcal_100g  \\\n",
       "0  HONIG stillende Frauen nicht geeignet. D bestr...               1.0   \n",
       "1  Sojaproteinisolat, Weizen - protein, Kaffee-Ex...             358.0   \n",
       "2  Water, Leptospermum Scoparium Mel (Manuka Hone...              45.0   \n",
       "3  Farine de blé 27%, chocolat au lait 18% (sucre...             460.0   \n",
       "4  Madeleines ChocoNoir - Madeleines nappées de c...             389.0   \n",
       "\n",
       "   fat_100g  saturated-fat_100g  carbohydrates_100g  sugars_100g  \\\n",
       "0       1.0                1.00                 1.0         1.00   \n",
       "1       2.0                0.50                 6.7         1.70   \n",
       "2      13.0                6.70                15.0         3.60   \n",
       "3      24.0                6.00                54.0        31.00   \n",
       "4      16.7                6.48                35.2         1.85   \n",
       "\n",
       "   proteins_100g  fiber_100g  salt_100g  sodium_100g healthy_level  \n",
       "0            1.0    1.000000     1.0000        0.400        Medium  \n",
       "1           76.0   10.714286     1.5000        0.600        Medium  \n",
       "2           11.0    0.000000     0.0625        0.025        Medium  \n",
       "3            6.4    1.400000     0.4800        0.192   Not Healthy  \n",
       "4           37.0   18.500000     0.8800        0.352        Medium  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset from .csv\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df = df.drop(columns=[\"nutriscore_score\", \"nutriscore_grade\"])\n",
    "print(\"Loaded:\", df.shape)\n",
    "\n",
    "# Clean dataset\n",
    "df = df[df[TEXT_COL].astype(str).str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5f16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = df[TEXT_COL].values\n",
    "x_num = df[NUM_COLS].values\n",
    "y = df[LABEL_COL].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(x_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf6a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text, x_test_text, x_train_num, x_test_num, y_train, y_test = train_test_split(\n",
    "    x_text, x_num, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c7ef4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "SEQ_LEN = 200\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LEN,\n",
    ")\n",
    "\n",
    "# Fit vectorizer\n",
    "vectorizer.adapt(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6e4f34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_6         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_12      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,184</span> │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_11[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_6         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,560,000\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_12      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ embedding_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m320\u001b[0m │ input_layer_12[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dense_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m5,184\u001b[0m │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,401</span> (9.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,576,401\u001b[0m (9.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,576,401</span> (9.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,576,401\u001b[0m (9.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# text_model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(1,), dtype=tf.string),\n",
    "#     vectorizer,\n",
    "#     tf.keras.layers.Embedding(VOCAB_SIZE, 128),\n",
    "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#     tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "# ])\n",
    "\n",
    "# num_model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(len(NUM_COLS),)),\n",
    "#     tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "#     tf.keras.layers.Dense(16, activation=\"relu\")\n",
    "# ])\n",
    "\n",
    "# # 1) Create input tensors\n",
    "text_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "num_input = tf.keras.Input(shape=(len(NUM_COLS),))\n",
    "\n",
    "# # 2) Call the sequential models on these inputs\n",
    "# text_output = text_model(text_input)\n",
    "# num_output = num_model(num_input)\n",
    "\n",
    "# # 3) Combine\n",
    "# combined = tf.keras.layers.Concatenate()([text_output, num_output])\n",
    "\n",
    "# z = tf.keras.layers.Dense(64, activation=\"relu\")(combined)\n",
    "# z = tf.keras.layers.Dense(32, activation=\"relu\")(z)\n",
    "# output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "# # 4) Build final model\n",
    "# model = tf.keras.Model(\n",
    "#     inputs=[text_input, num_input],\n",
    "#     outputs=output\n",
    "# )\n",
    "\n",
    "x = vectorizer(text_input)\n",
    "x = tf.keras.layers.Embedding(VOCAB_SIZE, 128)(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "y = tf.keras.layers.Dense(32, activation=\"relu\")(num_input)\n",
    "y = tf.keras.layers.Dense(16, activation=\"relu\")(y)\n",
    "\n",
    "combined = tf.keras.layers.Concatenate()([x, y])\n",
    "\n",
    "z = tf.keras.layers.Dense(64, activation=\"relu\")(combined)\n",
    "z = tf.keras.layers.Dense(32, activation=\"relu\")(z)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input, num_input], outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17e917bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing data for input \"input_layer_11\". You passed a data dictionary with keys ['text_input', 'num_input']. Expected the following keys: ['input_layer_11', 'input_layer_12']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_num\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_num\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 8. EVALUATE\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ==========================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#     y_test\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\anaconda3\\envs\\artificial_intelligence\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\anaconda3\\envs\\artificial_intelligence\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:149\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    150\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMissing data for input \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    151\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou passed a data dictionary with keys \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected the following keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m         )\n\u001b[32m    155\u001b[39m     list_inputs.append(inputs[name])\n\u001b[32m    156\u001b[39m inputs = list_inputs\n",
      "\u001b[31mValueError\u001b[39m: Missing data for input \"input_layer_11\". You passed a data dictionary with keys ['text_input', 'num_input']. Expected the following keys: ['input_layer_11', 'input_layer_12']"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    {\n",
    "        \"text_input\": x_train_text,\n",
    "        \"num_input\": x_train_num\n",
    "    },\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        {\n",
    "            \"text_input\": x_test_text,\n",
    "            \"num_input\": x_test_num\n",
    "        },\n",
    "        y_test\n",
    "    ),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8. EVALUATE\n",
    "# ==========================\n",
    "# model.evaluate(\n",
    "#     {\n",
    "#         \"input_layer_9\": x_test_text,\n",
    "#         \"input_layer_10\": x_test_num\n",
    "#     },\n",
    "#     y_test\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a44e94",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The layer sequential_6 has never been called and thus has no defined output.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m text_model = tf.keras.Sequential([\n\u001b[32m      2\u001b[39m     tf.keras.layers.Input(shape=(\u001b[32m1\u001b[39m,), dtype=tf.string),\n\u001b[32m      3\u001b[39m     vectorizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     tf.keras.layers.Dense(\u001b[32m64\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m ])\n\u001b[32m      9\u001b[39m num_model = tf.keras.Sequential([\n\u001b[32m     10\u001b[39m     tf.keras.layers.Input(shape=(\u001b[38;5;28mlen\u001b[39m(NUM_COLS),)),\n\u001b[32m     11\u001b[39m     tf.keras.layers.Dense(\u001b[32m32\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m     tf.keras.layers.Dense(\u001b[32m16\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m combined = tf.keras.layers.Concatenate()([\u001b[43mtext_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m, num_model.output])\n\u001b[32m     17\u001b[39m z = tf.keras.layers.Dense(\u001b[32m64\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m)(combined)\n\u001b[32m     18\u001b[39m z = tf.keras.layers.Dense(\u001b[32m32\u001b[39m, activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m)(z)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\anaconda3\\envs\\artificial_intelligence\\Lib\\site-packages\\keras\\src\\ops\\operation.py:349\u001b[39m, in \u001b[36mOperation.output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moutput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieves the output tensor(s) of a layer.\u001b[39;00m\n\u001b[32m    342\u001b[39m \n\u001b[32m    343\u001b[39m \u001b[33;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    347\u001b[39m \u001b[33;03m        Output tensor or list of output tensors.\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_tensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\anaconda3\\envs\\artificial_intelligence\\Lib\\site-packages\\keras\\src\\ops\\operation.py:368\u001b[39m, in \u001b[36mOperation._get_node_attribute_at_index\u001b[39m\u001b[34m(self, node_index, attr, attr_name)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[32m    353\u001b[39m \n\u001b[32m    354\u001b[39m \u001b[33;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m \u001b[33;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inbound_nodes:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    369\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has never been called \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    370\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m     )\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes) > node_index:\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    374\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at node \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    375\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but the operation has only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    376\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m inbound nodes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: The layer sequential_6 has never been called and thus has no defined output."
     ]
    }
   ],
   "source": [
    "\n",
    "])\n",
    "\n",
    "combined = tf.keras.layers.Concatenate()([text_model.output, num_model.output])\n",
    "\n",
    "z = tf.keras.layers.Dense(64, activation=\"relu\")(combined)\n",
    "z = tf.keras.layers.Dense(32, activation=\"relu\")(z)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[text_model.input, num_model.input], outputs=output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.Seq\n",
    "\n",
    "# === Prepare X and y ===\n",
    "X = df[TEXT_COL].astype(str)\n",
    "y = df[LABEL_COL].astype(str)\n",
    "\n",
    "# Encode labels to integers\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", list(le.classes_), \" -> num_classes:\", num_classes)\n",
    "\n",
    "# Stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=SEED\n",
    ")\n",
    "print(\"Train / Test sizes:\", len(X_train), len(X_test))\n",
    "\n",
    "# Optionally compute class weights if classes are imbalanced\n",
    "class_weights = None\n",
    "try:\n",
    "    cw = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = {i: float(w) for i, w in enumerate(cw)}\n",
    "    print(\"Class weights:\", class_weights)\n",
    "except Exception as e:\n",
    "    print(\"Could not compute class weight:\", e)\n",
    "\n",
    "# === TextVectorization and vocabulary ===\n",
    "vectorizer = TextVectorization(max_tokens=MAX_TOKENS, output_sequence_length=SEQ_LEN)\n",
    "# adapt uses a tf dataset or numpy array of strings\n",
    "vectorizer.adapt(X_train.values)\n",
    "\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# === Build the model ===\n",
    "model = tf.keras.Sequential([\n",
    "    vectorizer,\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIM),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# === Train ===\n",
    "history = model.fit(\n",
    "    X_train.values,\n",
    "    y_train,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weights  # set to None if you don't want to use it\n",
    ")\n",
    "\n",
    "# === Evaluate ===\n",
    "loss, acc = model.evaluate(X_test.values, y_test, batch_size=BATCH_SIZE)\n",
    "print(\"Test loss:\", loss, \" Test accuracy:\", acc)\n",
    "\n",
    "# Predictions and classification report\n",
    "y_prob = model.predict(X_test.values, batch_size=BATCH_SIZE)\n",
    "y_pred = np.argmax(y_prob, axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# === Save model and label encoder ===\n",
    "model_save_path = \"text_classifier_tf.keras\"\n",
    "le_save_path = \"label_encoder.joblib\"\n",
    "model.save(model_save_path)\n",
    "joblib.dump(le, le_save_path)\n",
    "print(f\"Saved model -> {model_save_path}, label encoder -> {le_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf374fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "model = tf.keras.models.load_model(\"text_classifier_tf.keras\")\n",
    "le = joblib.load(\"label_encoder.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"acid, sugar, salt, palm oil, wheat flour, fruit\"\n",
    "\n",
    "# Convert to numpy array so TensorFlow accepts it\n",
    "# x = np.array([text], dtype=tf.string)\n",
    "x = tf.constant([text], dtype=tf.string)\n",
    "\n",
    "pred_prob = model.predict(x)\n",
    "pred_class = np.argmax(pred_prob, axis=1)[0]\n",
    "label = le.inverse_transform([pred_class])[0]\n",
    "\n",
    "print(\"Predicted class:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3f21c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category order: ['Healthy', 'Medium', 'Not Healthy']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Embedding, TextVectorization, LSTM, Concatenate, Dropout, Normalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===========================\n",
    "# 1. LOAD DATA\n",
    "# ===========================\n",
    "TEXT_COL = \"ingredients_text\"\n",
    "NUM_COLS = [\n",
    "    \"energy-kcal_100g\",\n",
    "    \"fat_100g\",\n",
    "    \"saturated-fat_100g\",\n",
    "    \"carbohydrates_100g\",\n",
    "    \"sugars_100g\",\n",
    "    \"proteins_100g\",\n",
    "    \"fiber_100g\",\n",
    "    \"salt_100g\",\n",
    "    \"sodium_100g\"\n",
    "]\n",
    "LABEL_COL = \"healthy_level\"\n",
    "\n",
    "df = pd.read_csv(\"./dataset/filtered_data.csv\")\n",
    "\n",
    "# Remove rows with missing text or numeric data\n",
    "df = df.dropna(subset=[TEXT_COL] + NUM_COLS + [LABEL_COL])\n",
    "\n",
    "cat = df[LABEL_COL].astype(\"category\")\n",
    "print(\"Category order:\", list(cat.cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2d007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artificial_intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
